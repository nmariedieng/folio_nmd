{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c544eb83",
   "metadata": {},
   "source": [
    "# Notebook 5 — Classical time series models (SARIMA & Prophet)\n",
    "\n",
    "This notebook focuses on classical time series models applied to the French hourly electricity load:\n",
    "\n",
    "- SARIMA (from `statsmodels`)\n",
    "- Prophet (from Meta)\n",
    "\n",
    "In contrast with Notebook 4 (which uses feature-based machine learning models such as Ridge, Random forest, and XGBoost), this notebook works directly with the time series structure of the data.\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Load and prepare a clean, regular hourly time series.\n",
    "2. Fit a SARIMA model and evaluate its forecasting performance.\n",
    "3. Fit a Prophet model and evaluate its forecasting performance.\n",
    "4. Compare these classical models to the machine learning models from Notebook 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8510761",
   "metadata": {},
   "source": [
    "## 1. Load and prepare the base time series\n",
    "\n",
    "We first load the cleaned hourly load series used throughout the project.\n",
    "\n",
    "Steps:\n",
    "- Load `datetime` and `load_mw`.\n",
    "- Convert timestamps to UTC to avoid daylight savings time issues.\n",
    "- Remove timezone information (naive `datetime64[ns]` index).\n",
    "- Ensure a strictly hourly frequency.\n",
    "- Interpolate occasional missing hours to obtain a fully regular series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c30bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 50357 entries, 2015-01-01 01:00:00 to 2020-09-30 22:00:00\n",
      "Data columns (total 1 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   load_mw  50357 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 786.8 KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load_mw</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime_utc</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 01:00:00</th>\n",
       "      <td>69773.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00:00</th>\n",
       "      <td>66417.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 03:00:00</th>\n",
       "      <td>64182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 04:00:00</th>\n",
       "      <td>63859.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 05:00:00</th>\n",
       "      <td>63921.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     load_mw\n",
       "datetime_utc                \n",
       "2015-01-01 01:00:00  69773.0\n",
       "2015-01-01 02:00:00  66417.0\n",
       "2015-01-01 03:00:00  64182.0\n",
       "2015-01-01 04:00:00  63859.0\n",
       "2015-01-01 05:00:00  63921.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the base dataset (from previous notebooks)\n",
    "df_raw = pd.read_csv(\"../data/energy_france.csv\")\n",
    "\n",
    "# Parse datetime with Europe/Paris timezone\n",
    "df_raw[\"datetime\"] = pd.to_datetime(df_raw[\"datetime\"], utc=True).dt.tz_convert(\"Europe/Paris\")\n",
    "\n",
    "# Keep only what we need for classical TS models\n",
    "df_ts = df_raw[[\"datetime\", \"load_mw\"]].copy()\n",
    "\n",
    "# Convert to UTC and drop timezone for regular time series modeling\n",
    "df_ts[\"datetime_utc\"] = df_ts[\"datetime\"].dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "\n",
    "# Set index to UTC datetime\n",
    "df_ts = df_ts.set_index(\"datetime_utc\").sort_index()\n",
    "\n",
    "df_ts = df_ts[[\"load_mw\"]]  # keep only target\n",
    "\n",
    "print(df_ts.info())\n",
    "df_ts.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c611a8e9",
   "metadata": {},
   "source": [
    "We now have a single time series:\n",
    "\n",
    "- index: `datetime_utc` (naive `datetime64[ns]`, hourly)\n",
    "- column: `load_mw` (float), the national electricity demand in MW.\n",
    "\n",
    "Next, we must verify that the series is strictly hourly with no missing timestamps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c0ef56",
   "metadata": {},
   "source": [
    "## 2. Check regular hourly frequency and fill missing timestamps\n",
    "\n",
    "Classical time series models such as SARIMA and Prophet assume a regular time grid.\n",
    "\n",
    "We:\n",
    "- reindex the series on a strict hourly range,\n",
    "- detect missing timestamps,\n",
    "- fill them by interpolation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ba703c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length: 50357\n",
      "Full expected length: 50398\n",
      "Missing timestamps after reindex: 41\n",
      "Missing after interpolation: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load_mw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 01:00:00</th>\n",
       "      <td>69773.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00:00</th>\n",
       "      <td>66417.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 03:00:00</th>\n",
       "      <td>64182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 04:00:00</th>\n",
       "      <td>63859.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 05:00:00</th>\n",
       "      <td>63921.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     load_mw\n",
       "2015-01-01 01:00:00  69773.0\n",
       "2015-01-01 02:00:00  66417.0\n",
       "2015-01-01 03:00:00  64182.0\n",
       "2015-01-01 04:00:00  63859.0\n",
       "2015-01-01 05:00:00  63921.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a complete hourly index from min to max date (UTC)\n",
    "full_index = pd.date_range(\n",
    "    start=df_ts.index.min(),\n",
    "    end=df_ts.index.max(),\n",
    "    freq=\"h\"\n",
    ")\n",
    "\n",
    "print(\"Original length:\", len(df_ts))\n",
    "print(\"Full expected length:\", len(full_index))\n",
    "\n",
    "# Reindex to the full range\n",
    "df_ts_full = df_ts.reindex(full_index)\n",
    "\n",
    "missing_count = df_ts_full[\"load_mw\"].isna().sum()\n",
    "print(\"Missing timestamps after reindex:\", missing_count)\n",
    "\n",
    "# Interpolate missing values linearly\n",
    "df_ts_full[\"load_mw\"] = df_ts_full[\"load_mw\"].interpolate(\n",
    "    method=\"linear\",\n",
    "    limit_direction=\"both\"\n",
    ")\n",
    "\n",
    "print(\"Missing after interpolation:\", df_ts_full[\"load_mw\"].isna().sum())\n",
    "\n",
    "df_ts_full.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ffbe42",
   "metadata": {},
   "source": [
    "The reindexed series now has:\n",
    "\n",
    "- a perfectly regular hourly index,\n",
    "- no missing values after interpolation.\n",
    "\n",
    "This series is ready to be used with SARIMA and Prophet.\n",
    "For the rest of this notebook, `df_ts_full[\"load_mw\"]` is our target time series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498c9f9",
   "metadata": {},
   "source": [
    "## 3. Train/validation split (time-based)\n",
    "\n",
    "To evaluate classical time series models in a realistic scenario, \n",
    "we split the data chronologically:\n",
    "\n",
    "- Training period: from the beginning of the series up to 2018-12-31\n",
    "- Validation period: from 2019-01-01 onward\n",
    "\n",
    "It is split chronologically rather than randomly because random splits would leak future information into the past and produce overly optimistic metrics.\n",
    "\n",
    "This setup ensures that all models are evaluated on truly unseen future data,\n",
    "reflecting a realistic forecasting scenario. The validation window spans 2019–2020,\n",
    "which includes a wide range of seasonal patterns and demand variations, making it a\n",
    "suitable benchmark for assessing forecasting performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c39003cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 35063\n",
      "Valid length: 15335\n",
      "Train period: 2015-01-01 01:00:00 -> 2018-12-31 23:00:00\n",
      "Valid period: 2019-01-01 00:00:00 -> 2020-09-30 22:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "SPLIT_DATE = datetime(2019, 1, 1)\n",
    "\n",
    "y = df_ts_full[\"load_mw\"].copy()\n",
    "time_index = df_ts_full.index\n",
    "\n",
    "train_mask = time_index < SPLIT_DATE\n",
    "valid_mask = time_index >= SPLIT_DATE\n",
    "\n",
    "y_train = y.loc[train_mask]\n",
    "y_valid = y.loc[valid_mask]\n",
    "t_train = time_index[train_mask]\n",
    "t_valid = time_index[valid_mask]\n",
    "\n",
    "print(\"Train length:\", len(y_train))\n",
    "print(\"Valid length:\", len(y_valid))\n",
    "print(\"Train period:\", t_train.min(), \"->\", t_train.max())\n",
    "print(\"Valid period:\", t_valid.min(), \"->\", t_valid.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85129c5",
   "metadata": {},
   "source": [
    "## 4. SARIMA model\n",
    "\n",
    "We now fit a SARIMA model on the hourly load series.\n",
    "\n",
    "We use a seasonal ARIMA structure with daily seasonality:\n",
    "\n",
    "- Non-seasonal order: (p, d, q) = (1, 1, 1)\n",
    "- Seasonal order: (P, D, Q, s) = (1, 1, 1, 24)\n",
    "\n",
    "This specification is a reasonable starting point for hourly load data with a strong daily cycle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20ad005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>SARIMAX Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>               <td>load_mw</td>            <th>  No. Observations:  </th>    <td>35063</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>           <td>SARIMAX(1, 1, 1)x(1, 1, 1, 24)</td> <th>  Log Likelihood     </th> <td>-275608.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                   <td>Tue, 25 Nov 2025</td>        <th>  AIC                </th> <td>551226.162</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                       <td>08:56:35</td>            <th>  BIC                </th> <td>551268.479</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sample:</th>                    <td>01-01-2015</td>           <th>  HQIC               </th> <td>551239.641</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th>                          <td>- 12-31-2018</td>          <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>               <td>opg</td>              <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}   &            load\\_mw            & \\textbf{  No. Observations:  } &    35063     \\\\\n",
       "\\textbf{Model:}           & SARIMAX(1, 1, 1)x(1, 1, 1, 24) & \\textbf{  Log Likelihood     } & -275608.081  \\\\\n",
       "\\textbf{Date:}            &        Tue, 25 Nov 2025        & \\textbf{  AIC                } &  551226.162  \\\\\n",
       "\\textbf{Time:}            &            08:56:35            & \\textbf{  BIC                } &  551268.479  \\\\\n",
       "\\textbf{Sample:}          &           01-01-2015           & \\textbf{  HQIC               } &  551239.641  \\\\\n",
       "\\textbf{}                 &          - 12-31-2018          & \\textbf{                     } &              \\\\\n",
       "\\textbf{Covariance Type:} &              opg               & \\textbf{                     } &              \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{SARIMAX Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Define SARIMA structure\n",
    "sarima_order = (1, 1, 1)\n",
    "seasonal_order = (1, 1, 1, 24)\n",
    "\n",
    "# Train SARIMA on the training portion only\n",
    "sarima_model = sm.tsa.SARIMAX(\n",
    "    y_train,\n",
    "    order=sarima_order,\n",
    "    seasonal_order=seasonal_order,\n",
    "    enforce_stationarity=False,\n",
    "    enforce_invertibility=False\n",
    ")\n",
    "\n",
    "sarima_results = sarima_model.fit(disp=False)\n",
    "\n",
    "# Display only the summary header (model name + AIC/BIC)\n",
    "sarima_results.summary().tables[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af4f3df",
   "metadata": {},
   "source": [
    "**Interpretation of the SARIMAX model summary**\n",
    "\n",
    "The table above summarizes the fitted SARIMA(1,1,1)(1,1,1,24) model.\n",
    "\n",
    "Key points:\n",
    "- Number of observations (35,063) corresponds to the entire training period\n",
    "  from January 2015 to December 2018.\n",
    "- The model includes both non-seasonal and seasonal components:\n",
    "  - (1,1,1) captures short-term autocorrelation and trend differencing.\n",
    "  - (1,1,1,24) models the strong 24-hour seasonal cycle typical of hourly electricity load.\n",
    "- The **log-likelihood**, **AIC**, **BIC**, and **HQIC** values quantify the fit of the model.\n",
    "  These metrics are primarily useful when comparing different SARIMA configurations.\n",
    "  Lower values indicate a better statistical fit.\n",
    "- The covariance type “opg” refers to the method used to estimate parameter uncertainty.\n",
    "\n",
    "At this stage, the model fit is complete.  \n",
    "The next step is to generate out-of-sample forecasts for the validation period and\n",
    "evaluate how well SARIMA predicts actual electricity load values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af01865",
   "metadata": {},
   "source": [
    "### Forecasting the validation period\n",
    "  \n",
    "We now forecast the entire validation period starting on 2019-01-01.\n",
    "\n",
    "SARIMA generates forecasts based on internal time indexing, so we specify:\n",
    "- `start` = end of the training window\n",
    "- `end`   = end of the validation window\n",
    "\n",
    "The resulting forecast series will then be realigned with the timestamps of the\n",
    "validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b9211b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019-01-01 00:00:00    60150.231371\n",
       "2019-01-01 01:00:00    58756.889224\n",
       "2019-01-01 02:00:00    56142.649244\n",
       "2019-01-01 03:00:00    55081.090428\n",
       "2019-01-01 04:00:00    56392.138184\n",
       "Freq: h, Name: predicted_mean, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numeric range for SARIMA forecasting\n",
    "start = len(y_train)\n",
    "end = len(y_train) + len(y_valid) - 1\n",
    "\n",
    "# Forecast the entire validation period\n",
    "sarima_forecast = sarima_results.predict(start=start, end=end)\n",
    "\n",
    "# Align timestamps with the validation index\n",
    "sarima_forecast.index = y_valid.index\n",
    "\n",
    "# Preview the first few predictions\n",
    "sarima_forecast.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea95b4",
   "metadata": {},
   "source": [
    "**Interpretation of the first SARIMA predictions**\n",
    "\n",
    "The values above correspond to the first hourly forecasts produced by the\n",
    "SARIMA(1,1,1)(1,1,1,24) model for the start of the validation period\n",
    "beginning on 2019-01-01.\n",
    "\n",
    "Each value represents the model's estimate of the electricity load (in MW)\n",
    "for a specific hour.\n",
    "\n",
    "The next step is to evaluate these predictions quantitatively using RMSE\n",
    "and MAPE, and later to visualize them over a short time window to assess\n",
    "how closely SARIMA tracks the actual load curve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658b7c59",
   "metadata": {},
   "source": [
    "### Evaluation of SARIMA forecasts\n",
    "\n",
    "To assess the forecasting performance of the SARIMA model, we compare its predictions\n",
    "to the actual load values on the validation set using two standard metrics:\n",
    "\n",
    "- RMSE (Root Mean Squared Error)\n",
    "  Measures the average magnitude of forecast errors.  \n",
    "  Lower values indicate better predictive accuracy.\n",
    "\n",
    "- MAPE (Mean Absolute Percentage Error)  \n",
    "  Measures the average error relative to actual values, expressed as a percentage.  \n",
    "  This metric is intuitive because it shows how far predictions deviate from real load values in percentage terms.\n",
    "\n",
    "The goal of this step is to quantify how well SARIMA performs compared with\n",
    "our earlier baselines and machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12f95e",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(89093.740095992), np.float64(170.09942814149855))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define metrics\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((np.asarray(y_true) - np.asarray(y_pred))**2))\n",
    "\n",
    "def mape(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / (y_true + eps))) * 100\n",
    "\n",
    "# Compute metrics\n",
    "rmse_sarima = rmse(y_valid, sarima_forecast)\n",
    "mape_sarima = mape(y_valid, sarima_forecast)\n",
    "\n",
    "rmse_sarima, mape_sarima\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6a0f8d",
   "metadata": {},
   "source": [
    "**SARIMA performance analysis**\n",
    "\n",
    "The SARIMA(1,1,1)(1,1,1,24) model shows very poor forecasting performance on the\n",
    "validation set:\n",
    "\n",
    "- RMSE ≈ 89,094 MW  \n",
    "- MAPE ≈ 170 %\n",
    "\n",
    "These values indicate that the model is systematically far from the true load,\n",
    "with errors several times larger than the actual signal. This confirms that\n",
    "this SARIMA specification is not appropriate for our dataset.\n",
    "\n",
    "Such a result is not unusual for high-frequency electricity load data.\n",
    "Hourly load exhibits strong non-linearities, varying seasonal amplitudes,\n",
    "and structural changes that are difficult for classical SARIMA models to capture.\n",
    "Additionally, differencing the series (both regular and seasonal) can amplify\n",
    "noise and lead to unstable forecasts if the model is not carefully tuned.\n",
    "\n",
    "Overall, SARIMA provides a useful classical baseline, but in this case it is\n",
    "clearly outperformed by feature-based machine learning models such as\n",
    "Ridge regression, Random Forest, and XGBoost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c30205",
   "metadata": {},
   "source": [
    "## 5. Prophet modeling\n",
    "\n",
    "Prophet requires a specific data format with:\n",
    "- a datetime column named ds\n",
    "- a target column named y\n",
    "\n",
    "In this step, we convert our cleaned time series (`df_ts_full`) into Prophet’s\n",
    "required format. We keep the same train/validation split as in previous sections\n",
    "to ensure consistency when comparing forecasting performance across models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "148b1380",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (35063, 2)\n",
      "Valid: (15335, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 01:00:00</td>\n",
       "      <td>69773.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 02:00:00</td>\n",
       "      <td>66417.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 03:00:00</td>\n",
       "      <td>64182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 04:00:00</td>\n",
       "      <td>63859.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 05:00:00</td>\n",
       "      <td>63921.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ds        y\n",
       "0 2015-01-01 01:00:00  69773.0\n",
       "1 2015-01-01 02:00:00  66417.0\n",
       "2 2015-01-01 03:00:00  64182.0\n",
       "3 2015-01-01 04:00:00  63859.0\n",
       "4 2015-01-01 05:00:00  63921.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data in Prophet format\n",
    "df_prophet = df_ts_full.reset_index().rename(columns={\n",
    "    \"index\": \"ds\",\n",
    "    \"load_mw\": \"y\"\n",
    "})\n",
    "\n",
    "df_prophet[\"ds\"] = pd.to_datetime(df_prophet[\"ds\"])\n",
    "\n",
    "# Reapply the same split date\n",
    "from datetime import datetime\n",
    "\n",
    "SPLIT_DATE = datetime(2019, 1, 1)\n",
    "\n",
    "train_prophet = df_prophet[df_prophet[\"ds\"] < SPLIT_DATE]\n",
    "valid_prophet = df_prophet[df_prophet[\"ds\"] >= SPLIT_DATE]\n",
    "\n",
    "print(\"Train:\", train_prophet.shape)\n",
    "print(\"Valid:\", valid_prophet.shape)\n",
    "train_prophet.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56327a61",
   "metadata": {},
   "source": [
    "### Create and configure the Prophet model\n",
    "\n",
    "Prophet decomposes the time series into trend, multiple seasonalities, and\n",
    "holiday effects. Because our data is hourly, we explicitly enable:\n",
    "\n",
    "- yearly seasonality\n",
    "- weekly seasonality\n",
    "- daily seasonality (important for hourly electricity load)\n",
    "\n",
    "This configuration allows Prophet to model the main cycles present in the load\n",
    "series without manual feature engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "184f9717",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:36:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "09:37:21 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<prophet.forecaster.Prophet at 0x28453024c20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "# Create Prophet model with daily + weekly + yearly seasonality\n",
    "model_prophet = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=True,\n",
    "    changepoint_prior_scale=0.05  # default, controls trend flexibility\n",
    ")\n",
    "\n",
    "# Fit model on training data\n",
    "model_prophet.fit(train_prophet)\n",
    "\n",
    "model_prophet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5755514",
   "metadata": {},
   "source": [
    "### Forecasting the validation period with Prophet\n",
    "\n",
    "Prophet requires a DataFrame containing the future timestamps to be predicted.\n",
    "We therefore create a forecast frame using the `ds` column from the validation set,\n",
    "and ask Prophet to generate predictions for each timestamp.\n",
    "\n",
    "Prophet returns a full decomposition of the forecast (trend, seasonality, etc.).\n",
    "For evaluation, we will extract the `yhat` column, which contains the point forecast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21079b9e",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ds\n",
       "2019-01-01 00:00:00    60122.505777\n",
       "2019-01-01 01:00:00    57499.518586\n",
       "2019-01-01 02:00:00    56061.076262\n",
       "2019-01-01 03:00:00    56810.331670\n",
       "2019-01-01 04:00:00    59579.474810\n",
       "Name: yhat, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the prediction frame using the validation timestamps\n",
    "future_valid = valid_prophet[[\"ds\"]].copy()\n",
    "\n",
    "# Generate predictions\n",
    "forecast_prophet = model_prophet.predict(future_valid)\n",
    "\n",
    "# Extract only the yhat predictions and align index\n",
    "prophet_pred = forecast_prophet[[\"ds\", \"yhat\"]].set_index(\"ds\")[\"yhat\"]\n",
    "\n",
    "# Show first predictions\n",
    "prophet_pred.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e338ba",
   "metadata": {},
   "source": [
    "### Evaluation of Prophet forecasts\n",
    "\n",
    "We now compare Prophet’s predictions to the actual load values in the\n",
    "validation period using RMSE and MAPE.\n",
    "\n",
    "These metrics will allow us to assess:\n",
    "- how well Prophet captures the overall structure of the time series,\n",
    "- and how its accuracy compares with previous models such as SARIMA,\n",
    "  Ridge regression, Random Forest, and XGBoost.\n",
    "\n",
    "Because Prophet internally models daily, weekly, and yearly seasonalities,\n",
    "we expect it to outperform SARIMA and provide a competitive time-series-only baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "430d0a44",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(8437.601647853975), np.float64(15.163622229347837))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute RMSE and MAPE for Prophet\n",
    "rmse_prophet = rmse(valid_prophet[\"y\"], prophet_pred)\n",
    "mape_prophet = mape(valid_prophet[\"y\"], prophet_pred)\n",
    "\n",
    "rmse_prophet, mape_prophet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773a84d",
   "metadata": {},
   "source": [
    "### Interpretation of Prophet performance\n",
    "\n",
    "Prophet achieves the following results on the validation period:\n",
    "\n",
    "- RMSE ≈ 8,438 MW\n",
    "- MAPE ≈ 15.16 %\n",
    "\n",
    "These values represent a significant improvement over the SARIMA model, which\n",
    "performed very poorly on this dataset. Prophet’s ability to model multiple\n",
    "seasonalities (daily, weekly, yearly) allows it to capture part of the structure\n",
    "of the electricity load.\n",
    "\n",
    "However, the error remains relatively high compared with the feature-based\n",
    "machine learning models developed earlier (Ridge, Random Forest, XGBoost),\n",
    "which achieved MAPE values between 1 % and 2 %. This gap illustrates a key\n",
    "limitation of Prophet: although it is powerful for general time-series\n",
    "structures, it does not incorporate engineered lag features or rolling-window\n",
    "statistics, which are crucial for accurately modeling electricity demand.\n",
    "\n",
    "Prophet therefore provides a solid classical baseline, but not a state-of-the-art\n",
    "model for this specific forecasting task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
